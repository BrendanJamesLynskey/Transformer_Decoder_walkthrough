# The Transformer Decoder — Every Computation, From Scratch

An interactive, browser-based walkthrough of every operation inside a GPT-style autoregressive transformer decoder. All computations run live in your browser using typed arrays and arithmetic — no frameworks, no dependencies, no magic.

**[Launch the walkthrough →](https://brendanjameslynskey.github.io/Transformer_Decoder_walkthrough/)**

## Overview

This single-page application implements a complete transformer decoder from scratch in vanilla JavaScript, then walks you through each stage of the forward pass with real tensor values you can inspect. The model uses a modern LLaMA-style architecture with Grouped Query Attention, RoPE positional embeddings, SwiGLU activations, and RMSNorm.

### Model Configuration

| Parameter | Value |
|-----------|-------|
| Vocabulary size | 64 |
| Model dimension (d_model) | 32 |
| Attention heads | 4 |
| KV heads (GQA) | 2 |
| Head dimension | 8 |
| FFN hidden dim | 86 |
| Layers | 2 |

## Sections

The walkthrough is organised into 14 expandable sections, each covering a distinct stage of the decoder's forward pass:

0. **Architecture Overview** — Visual diagram of the full data flow from token IDs to next-token prediction, with cross-references to each computation stage.
1. **Token Embedding** — Lookup-table conversion from integer IDs to dense vectors.
2. **Rotary Positional Embeddings (RoPE)** — Pair-wise rotation of Q and K dimensions to encode position without learnable parameters.
3. **RMSNorm** — Root Mean Square Normalization, the simpler modern replacement for LayerNorm (no mean subtraction, no bias).
4. **QKV Projections & Grouped Query Attention (GQA)** — Linear projections to query, key and value spaces, with KV head sharing across query heads.
5. **Scaled Dot-Product Attention** — QKᵀ/√d scoring with causal masking, softmax, and weighted value combination. Includes per-head attention heatmaps.
6. **SwiGLU Feed-Forward Network** — Gated FFN with SiLU activation: SiLU(x·W_gate) ⊙ (x·W_up) → W_down.
7. **Residual Connections** — Skip connections around each sub-layer.
8. **LM Head (Logit Projection)** — Final RMSNorm followed by linear projection to vocabulary-sized logits.
9. **Sampling** — Temperature scaling, top-k filtering, and categorical sampling.
10. **Autoregressive Generation** — Interactive demo of the full generation loop with token-by-token output.
11. **KV-Cache** — Explanation of prefill vs. decode phases, O(n²) → O(n) savings, and memory scaling to LLaMA-7B.
12. **Computation & Memory Summary** — FLOPs breakdown per operation with scaling insight (attention O(n²·d) vs FFN O(n·d²)).
13. **Summary & Key Takeaways** — Seven core insights distilled from the walkthrough.

## How It Works

The entire implementation lives in a single `index.html` file:

- **Math engine** — Pure JavaScript `Float32Array`-based tensor operations: `matmul`, `bmm`, `softmax`, `rmsnorm`, `silu`, RoPE rotation, and causal masking.
- **Model initialisation** — Seeded random parameters (seed 42) using a simple LCG PRNG, giving reproducible results on every load.
- **Forward pass** — Full transformer block execution (RMSNorm → QKV → RoPE → GQA expand → attention → output projection → residual → RMSNorm → SwiGLU → residual) across all layers.
- **Rendering** — Collapsible sections built via vanilla DOM manipulation, with attention heatmaps, probability bar charts, and architecture diagrams.

## Running Locally

Simply open `index.html` in any modern browser — no build step, no server required:

```bash
git clone https://github.com/BrendanJamesLynskey/Transformer_Decoder_walkthrough.git
cd Transformer_Decoder_walkthrough
open index.html
```

Or serve it locally:

```bash
python3 -m http.server 8000
# Visit http://localhost:8000
```

## Built With

- Vanilla JavaScript (ES6+)
- `Float32Array` typed arrays for all tensor operations
- IBM Plex Mono & IBM Plex Sans (Google Fonts)
- Zero dependencies

## Credits

Code generated by [Claude](https://claude.ai) (Anthropic).

## Licence

This project is provided as-is for educational purposes.
